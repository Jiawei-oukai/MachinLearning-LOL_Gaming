{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras.optimizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pre = pd.read_csv(\"datasets/logisticRegression_feature_data/train_pre.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pre = data_train_pre['TeamVictory']\n",
    "X_train_pre = data_train_pre.drop('TeamVictory', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scale = X_train_pre.copy()\n",
    "scaler.fit(X_scale)\n",
    "X_scale = pd.DataFrame(scaler.transform(X_scale),columns= X_scale.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre_X, test_pre_X, train_pre_y, test_pre_y = train_test_split(X_scale, y_train_pre, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top_counterScore</th>\n",
       "      <th>Jug_counterScore</th>\n",
       "      <th>Mid_counterScore</th>\n",
       "      <th>Bot_counterScore</th>\n",
       "      <th>Uti_counterScore</th>\n",
       "      <th>Team1_Top_attackScore</th>\n",
       "      <th>Team1_Jug_attackScore</th>\n",
       "      <th>Team1_Mid_attackScore</th>\n",
       "      <th>Team1_Bot_attackScore</th>\n",
       "      <th>Team1_Uti_attackScore</th>\n",
       "      <th>...</th>\n",
       "      <th>Team1_Top_controlScore</th>\n",
       "      <th>Team1_Jug_controlScore</th>\n",
       "      <th>Team1_Mid_controlScore</th>\n",
       "      <th>Team1_Bot_controlScore</th>\n",
       "      <th>Team1_Uti_controlScore</th>\n",
       "      <th>Team2_Top_controlScore</th>\n",
       "      <th>Team2_Jug_controlScore</th>\n",
       "      <th>Team2_Mid_controlScore</th>\n",
       "      <th>Team2_Bot_controlScore</th>\n",
       "      <th>Team2_Uti_controlScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>-0.302729</td>\n",
       "      <td>0.530685</td>\n",
       "      <td>0.173728</td>\n",
       "      <td>-0.726400</td>\n",
       "      <td>-1.705119</td>\n",
       "      <td>-0.208317</td>\n",
       "      <td>0.263919</td>\n",
       "      <td>0.174820</td>\n",
       "      <td>1.026434</td>\n",
       "      <td>0.161020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082055</td>\n",
       "      <td>0.221226</td>\n",
       "      <td>-0.560212</td>\n",
       "      <td>-0.849214</td>\n",
       "      <td>-0.851014</td>\n",
       "      <td>0.582466</td>\n",
       "      <td>-0.258430</td>\n",
       "      <td>-1.299079</td>\n",
       "      <td>-0.457158</td>\n",
       "      <td>-1.344014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>0.023418</td>\n",
       "      <td>0.788775</td>\n",
       "      <td>0.173728</td>\n",
       "      <td>1.138878</td>\n",
       "      <td>1.590395</td>\n",
       "      <td>0.059302</td>\n",
       "      <td>0.540024</td>\n",
       "      <td>-1.825171</td>\n",
       "      <td>0.162256</td>\n",
       "      <td>1.775953</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.226611</td>\n",
       "      <td>-0.886329</td>\n",
       "      <td>1.564375</td>\n",
       "      <td>0.380781</td>\n",
       "      <td>-0.246186</td>\n",
       "      <td>1.811398</td>\n",
       "      <td>-0.093473</td>\n",
       "      <td>-0.336892</td>\n",
       "      <td>-0.993911</td>\n",
       "      <td>-0.477678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>-0.847574</td>\n",
       "      <td>0.187975</td>\n",
       "      <td>-0.554878</td>\n",
       "      <td>0.068896</td>\n",
       "      <td>0.108098</td>\n",
       "      <td>0.257563</td>\n",
       "      <td>-0.493255</td>\n",
       "      <td>-0.446572</td>\n",
       "      <td>-0.192890</td>\n",
       "      <td>-1.102268</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034664</td>\n",
       "      <td>-0.608673</td>\n",
       "      <td>1.555409</td>\n",
       "      <td>-0.458213</td>\n",
       "      <td>1.334496</td>\n",
       "      <td>-0.340985</td>\n",
       "      <td>1.269395</td>\n",
       "      <td>0.475254</td>\n",
       "      <td>-0.888057</td>\n",
       "      <td>0.285049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>-0.863480</td>\n",
       "      <td>2.729249</td>\n",
       "      <td>-0.876378</td>\n",
       "      <td>-0.144168</td>\n",
       "      <td>-0.480671</td>\n",
       "      <td>0.819139</td>\n",
       "      <td>-1.222818</td>\n",
       "      <td>0.908814</td>\n",
       "      <td>-1.258955</td>\n",
       "      <td>-0.414668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.516515</td>\n",
       "      <td>1.248754</td>\n",
       "      <td>0.961371</td>\n",
       "      <td>1.234572</td>\n",
       "      <td>-1.760104</td>\n",
       "      <td>-1.006710</td>\n",
       "      <td>-0.200027</td>\n",
       "      <td>0.114465</td>\n",
       "      <td>-0.888057</td>\n",
       "      <td>1.301501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7974</th>\n",
       "      <td>0.023418</td>\n",
       "      <td>1.111492</td>\n",
       "      <td>0.173728</td>\n",
       "      <td>1.184802</td>\n",
       "      <td>0.412475</td>\n",
       "      <td>0.379595</td>\n",
       "      <td>-0.002532</td>\n",
       "      <td>0.074517</td>\n",
       "      <td>1.899239</td>\n",
       "      <td>0.932554</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.007588</td>\n",
       "      <td>-0.035656</td>\n",
       "      <td>0.886898</td>\n",
       "      <td>-0.282304</td>\n",
       "      <td>-0.515878</td>\n",
       "      <td>0.750130</td>\n",
       "      <td>-1.031222</td>\n",
       "      <td>-1.609279</td>\n",
       "      <td>-0.993911</td>\n",
       "      <td>-1.344014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9225</th>\n",
       "      <td>-1.446003</td>\n",
       "      <td>0.916581</td>\n",
       "      <td>-0.979286</td>\n",
       "      <td>-0.493682</td>\n",
       "      <td>-1.030579</td>\n",
       "      <td>-0.070909</td>\n",
       "      <td>-0.595905</td>\n",
       "      <td>-0.853846</td>\n",
       "      <td>-2.364393</td>\n",
       "      <td>1.775953</td>\n",
       "      <td>...</td>\n",
       "      <td>1.021514</td>\n",
       "      <td>-0.262009</td>\n",
       "      <td>-0.273530</td>\n",
       "      <td>1.651055</td>\n",
       "      <td>-0.246186</td>\n",
       "      <td>-1.497245</td>\n",
       "      <td>-0.258430</td>\n",
       "      <td>0.172458</td>\n",
       "      <td>0.259160</td>\n",
       "      <td>-0.535813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4859</th>\n",
       "      <td>1.303363</td>\n",
       "      <td>-0.354226</td>\n",
       "      <td>0.173728</td>\n",
       "      <td>-0.642439</td>\n",
       "      <td>-0.665853</td>\n",
       "      <td>0.234662</td>\n",
       "      <td>-0.595905</td>\n",
       "      <td>-2.477226</td>\n",
       "      <td>-2.364393</td>\n",
       "      <td>-0.414668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.521638</td>\n",
       "      <td>-0.262009</td>\n",
       "      <td>1.220157</td>\n",
       "      <td>1.651055</td>\n",
       "      <td>-1.760104</td>\n",
       "      <td>0.087260</td>\n",
       "      <td>0.363136</td>\n",
       "      <td>1.944323</td>\n",
       "      <td>-0.457158</td>\n",
       "      <td>0.471597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>0.023418</td>\n",
       "      <td>-0.552668</td>\n",
       "      <td>0.173728</td>\n",
       "      <td>-0.726400</td>\n",
       "      <td>0.015239</td>\n",
       "      <td>-0.531718</td>\n",
       "      <td>-0.228329</td>\n",
       "      <td>-0.337781</td>\n",
       "      <td>1.026434</td>\n",
       "      <td>-0.731532</td>\n",
       "      <td>...</td>\n",
       "      <td>1.404997</td>\n",
       "      <td>-0.642815</td>\n",
       "      <td>-0.139031</td>\n",
       "      <td>-0.849214</td>\n",
       "      <td>2.120087</td>\n",
       "      <td>0.087260</td>\n",
       "      <td>0.510700</td>\n",
       "      <td>-1.254906</td>\n",
       "      <td>-0.457158</td>\n",
       "      <td>-1.344014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9845</th>\n",
       "      <td>-0.416061</td>\n",
       "      <td>0.187975</td>\n",
       "      <td>0.173728</td>\n",
       "      <td>1.184802</td>\n",
       "      <td>0.694579</td>\n",
       "      <td>0.559535</td>\n",
       "      <td>0.944697</td>\n",
       "      <td>1.399067</td>\n",
       "      <td>1.899239</td>\n",
       "      <td>-0.504856</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.650824</td>\n",
       "      <td>1.115796</td>\n",
       "      <td>0.500587</td>\n",
       "      <td>-0.282304</td>\n",
       "      <td>0.513016</td>\n",
       "      <td>-0.068466</td>\n",
       "      <td>-0.200027</td>\n",
       "      <td>0.306458</td>\n",
       "      <td>-0.993911</td>\n",
       "      <td>2.045108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>1.468759</td>\n",
       "      <td>-0.818766</td>\n",
       "      <td>-0.375355</td>\n",
       "      <td>0.602637</td>\n",
       "      <td>0.108098</td>\n",
       "      <td>0.216341</td>\n",
       "      <td>-0.002532</td>\n",
       "      <td>-0.314394</td>\n",
       "      <td>-1.457077</td>\n",
       "      <td>-0.537222</td>\n",
       "      <td>...</td>\n",
       "      <td>1.593792</td>\n",
       "      <td>-0.035656</td>\n",
       "      <td>0.136443</td>\n",
       "      <td>3.475910</td>\n",
       "      <td>-1.341316</td>\n",
       "      <td>-1.026176</td>\n",
       "      <td>0.363136</td>\n",
       "      <td>-0.659184</td>\n",
       "      <td>-0.292834</td>\n",
       "      <td>-1.234721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7880 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Top_counterScore  Jug_counterScore  Mid_counterScore  Bot_counterScore  \\\n",
       "1525         -0.302729          0.530685          0.173728         -0.726400   \n",
       "1837          0.023418          0.788775          0.173728          1.138878   \n",
       "6917         -0.847574          0.187975         -0.554878          0.068896   \n",
       "3172         -0.863480          2.729249         -0.876378         -0.144168   \n",
       "7974          0.023418          1.111492          0.173728          1.184802   \n",
       "...                ...               ...               ...               ...   \n",
       "9225         -1.446003          0.916581         -0.979286         -0.493682   \n",
       "4859          1.303363         -0.354226          0.173728         -0.642439   \n",
       "3264          0.023418         -0.552668          0.173728         -0.726400   \n",
       "9845         -0.416061          0.187975          0.173728          1.184802   \n",
       "2732          1.468759         -0.818766         -0.375355          0.602637   \n",
       "\n",
       "      Uti_counterScore  Team1_Top_attackScore  Team1_Jug_attackScore  \\\n",
       "1525         -1.705119              -0.208317               0.263919   \n",
       "1837          1.590395               0.059302               0.540024   \n",
       "6917          0.108098               0.257563              -0.493255   \n",
       "3172         -0.480671               0.819139              -1.222818   \n",
       "7974          0.412475               0.379595              -0.002532   \n",
       "...                ...                    ...                    ...   \n",
       "9225         -1.030579              -0.070909              -0.595905   \n",
       "4859         -0.665853               0.234662              -0.595905   \n",
       "3264          0.015239              -0.531718              -0.228329   \n",
       "9845          0.694579               0.559535               0.944697   \n",
       "2732          0.108098               0.216341              -0.002532   \n",
       "\n",
       "      Team1_Mid_attackScore  Team1_Bot_attackScore  Team1_Uti_attackScore  \\\n",
       "1525               0.174820               1.026434               0.161020   \n",
       "1837              -1.825171               0.162256               1.775953   \n",
       "6917              -0.446572              -0.192890              -1.102268   \n",
       "3172               0.908814              -1.258955              -0.414668   \n",
       "7974               0.074517               1.899239               0.932554   \n",
       "...                     ...                    ...                    ...   \n",
       "9225              -0.853846              -2.364393               1.775953   \n",
       "4859              -2.477226              -2.364393              -0.414668   \n",
       "3264              -0.337781               1.026434              -0.731532   \n",
       "9845               1.399067               1.899239              -0.504856   \n",
       "2732              -0.314394              -1.457077              -0.537222   \n",
       "\n",
       "      ...  Team1_Top_controlScore  Team1_Jug_controlScore  \\\n",
       "1525  ...               -0.082055                0.221226   \n",
       "1837  ...               -1.226611               -0.886329   \n",
       "6917  ...               -0.034664               -0.608673   \n",
       "3172  ...               -0.516515                1.248754   \n",
       "7974  ...               -1.007588               -0.035656   \n",
       "...   ...                     ...                     ...   \n",
       "9225  ...                1.021514               -0.262009   \n",
       "4859  ...               -0.521638               -0.262009   \n",
       "3264  ...                1.404997               -0.642815   \n",
       "9845  ...               -1.650824                1.115796   \n",
       "2732  ...                1.593792               -0.035656   \n",
       "\n",
       "      Team1_Mid_controlScore  Team1_Bot_controlScore  Team1_Uti_controlScore  \\\n",
       "1525               -0.560212               -0.849214               -0.851014   \n",
       "1837                1.564375                0.380781               -0.246186   \n",
       "6917                1.555409               -0.458213                1.334496   \n",
       "3172                0.961371                1.234572               -1.760104   \n",
       "7974                0.886898               -0.282304               -0.515878   \n",
       "...                      ...                     ...                     ...   \n",
       "9225               -0.273530                1.651055               -0.246186   \n",
       "4859                1.220157                1.651055               -1.760104   \n",
       "3264               -0.139031               -0.849214                2.120087   \n",
       "9845                0.500587               -0.282304                0.513016   \n",
       "2732                0.136443                3.475910               -1.341316   \n",
       "\n",
       "      Team2_Top_controlScore  Team2_Jug_controlScore  Team2_Mid_controlScore  \\\n",
       "1525                0.582466               -0.258430               -1.299079   \n",
       "1837                1.811398               -0.093473               -0.336892   \n",
       "6917               -0.340985                1.269395                0.475254   \n",
       "3172               -1.006710               -0.200027                0.114465   \n",
       "7974                0.750130               -1.031222               -1.609279   \n",
       "...                      ...                     ...                     ...   \n",
       "9225               -1.497245               -0.258430                0.172458   \n",
       "4859                0.087260                0.363136                1.944323   \n",
       "3264                0.087260                0.510700               -1.254906   \n",
       "9845               -0.068466               -0.200027                0.306458   \n",
       "2732               -1.026176                0.363136               -0.659184   \n",
       "\n",
       "      Team2_Bot_controlScore  Team2_Uti_controlScore  \n",
       "1525               -0.457158               -1.344014  \n",
       "1837               -0.993911               -0.477678  \n",
       "6917               -0.888057                0.285049  \n",
       "3172               -0.888057                1.301501  \n",
       "7974               -0.993911               -1.344014  \n",
       "...                      ...                     ...  \n",
       "9225                0.259160               -0.535813  \n",
       "4859               -0.457158                0.471597  \n",
       "3264               -0.457158               -1.344014  \n",
       "9845               -0.993911                2.045108  \n",
       "2732               -0.292834               -1.234721  \n",
       "\n",
       "[7880 rows x 47 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6137055837563452"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = LogisticRegression()\n",
    "# model.fit(train_pre_X, train_pre_y)\n",
    "# y_predict = model.predict(test_pre_X)\n",
    "# accuracy_score(test_pre_y, y_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pre_X = tf.cast(train_pre_X, tf.float32)\n",
    "# test_pre_X = tf.cast(test_pre_X, tf.float32)\n",
    "# y_train = tf.one_hot(train_pre_y, depth=2)\n",
    "# y_test = tf.one_hot(test_pre_y, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "788/788 [==============================] - 1s 853us/step - loss: -20862326.0000 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "788/788 [==============================] - 1s 792us/step - loss: -1403808256.0000 - accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "788/788 [==============================] - 1s 814us/step - loss: -12141539328.0000 - accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "788/788 [==============================] - 1s 803us/step - loss: -48614653952.0000 - accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "788/788 [==============================] - 1s 897us/step - loss: -134088212480.0000 - accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "788/788 [==============================] - 1s 805us/step - loss: -296954789888.0000 - accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "788/788 [==============================] - 1s 835us/step - loss: -570087178240.0000 - accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "788/788 [==============================] - 1s 793us/step - loss: -990202429440.0000 - accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "788/788 [==============================] - 1s 827us/step - loss: -1602067890176.0000 - accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "788/788 [==============================] - 1s 822us/step - loss: -2452689780736.0000 - accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "788/788 [==============================] - 1s 799us/step - loss: -3596715556864.0000 - accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "788/788 [==============================] - 1s 785us/step - loss: -5097831006208.0000 - accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "788/788 [==============================] - 1s 792us/step - loss: -7022440349696.0000 - accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "788/788 [==============================] - 1s 783us/step - loss: -9448618721280.0000 - accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "788/788 [==============================] - 1s 809us/step - loss: -12448054640640.0000 - accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "788/788 [==============================] - 1s 824us/step - loss: -16130029649920.0000 - accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "788/788 [==============================] - 1s 804us/step - loss: -20580782833664.0000 - accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "788/788 [==============================] - 1s 794us/step - loss: -25894391382016.0000 - accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "788/788 [==============================] - 1s 798us/step - loss: -32194558951424.0000 - accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "788/788 [==============================] - 1s 813us/step - loss: -39610851786752.0000 - accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "788/788 [==============================] - 1s 824us/step - loss: -48276376125440.0000 - accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "788/788 [==============================] - 1s 804us/step - loss: -58324515028992.0000 - accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "788/788 [==============================] - 1s 823us/step - loss: -69874302844928.0000 - accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "788/788 [==============================] - 1s 805us/step - loss: -83127565287424.0000 - accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "788/788 [==============================] - 1s 802us/step - loss: -98244088561664.0000 - accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "788/788 [==============================] - 1s 782us/step - loss: -115401878929408.0000 - accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "788/788 [==============================] - 1s 781us/step - loss: -134757333598208.0000 - accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "788/788 [==============================] - 1s 773us/step - loss: -156520419426304.0000 - accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "788/788 [==============================] - 1s 779us/step - loss: -180888352587776.0000 - accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "788/788 [==============================] - 1s 784us/step - loss: -208117908373504.0000 - accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "788/788 [==============================] - 1s 774us/step - loss: -238449537843200.0000 - accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "788/788 [==============================] - 1s 771us/step - loss: -272044721700864.0000 - accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "788/788 [==============================] - 1s 783us/step - loss: -309178891501568.0000 - accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "788/788 [==============================] - 1s 800us/step - loss: -350052014882816.0000 - accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "788/788 [==============================] - 1s 789us/step - loss: -395145614721024.0000 - accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "788/788 [==============================] - 1s 785us/step - loss: -444687223619584.0000 - accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "788/788 [==============================] - 1s 819us/step - loss: -498901119401984.0000 - accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "788/788 [==============================] - 1s 784us/step - loss: -558082648178688.0000 - accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "788/788 [==============================] - 1s 848us/step - loss: -622452363231232.0000 - accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "788/788 [==============================] - 1s 783us/step - loss: -692530257592320.0000 - accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "788/788 [==============================] - 1s 799us/step - loss: -768784314925056.0000 - accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "788/788 [==============================] - 1s 790us/step - loss: -851326271488000.0000 - accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "788/788 [==============================] - 1s 795us/step - loss: -940562404343808.0000 - accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "788/788 [==============================] - 1s 780us/step - loss: -1037085888741376.0000 - accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "788/788 [==============================] - 1s 801us/step - loss: -1141098856579072.0000 - accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "788/788 [==============================] - 1s 824us/step - loss: -1252883165085696.0000 - accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "788/788 [==============================] - 1s 809us/step - loss: -1372938372644864.0000 - accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "788/788 [==============================] - 1s 793us/step - loss: -1502225478189056.0000 - accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "788/788 [==============================] - 1s 781us/step - loss: -1640803671736320.0000 - accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "788/788 [==============================] - 1s 770us/step - loss: -1789373838262272.0000 - accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "788/788 [==============================] - 1s 829us/step - loss: -1948072477196288.0000 - accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "788/788 [==============================] - 1s 812us/step - loss: -2117598191812608.0000 - accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "788/788 [==============================] - 1s 798us/step - loss: -2298593482375168.0000 - accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "788/788 [==============================] - 1s 798us/step - loss: -2491195385184256.0000 - accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "788/788 [==============================] - 1s 803us/step - loss: -2696660715044864.0000 - accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "788/788 [==============================] - 1s 808us/step - loss: -2915214689304576.0000 - accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "788/788 [==============================] - 1s 798us/step - loss: -3147233386037248.0000 - accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "788/788 [==============================] - 1s 804us/step - loss: -3393633512325120.0000 - accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "788/788 [==============================] - 1s 831us/step - loss: -3654591161827328.0000 - accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "788/788 [==============================] - 1s 848us/step - loss: -3931115383422976.0000 - accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "788/788 [==============================] - 1s 861us/step - loss: -4224542180376576.0000 - accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "788/788 [==============================] - 1s 803us/step - loss: -4534528223739904.0000 - accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "788/788 [==============================] - 1s 880us/step - loss: -4862474209722368.0000 - accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "788/788 [==============================] - 1s 846us/step - loss: -5208205118406656.0000 - accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "788/788 [==============================] - 1s 865us/step - loss: -5573274117341184.0000 - accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "788/788 [==============================] - 1s 798us/step - loss: -5958218614308864.0000 - accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "788/788 [==============================] - 1s 786us/step - loss: -6363988870823936.0000 - accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "788/788 [==============================] - 1s 824us/step - loss: -6792015647866880.0000 - accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "788/788 [==============================] - 1s 816us/step - loss: -7241323450990592.0000 - accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "788/788 [==============================] - 1s 821us/step - loss: -7712074952081408.0000 - accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "788/788 [==============================] - 1s 850us/step - loss: -8208503915151360.0000 - accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "788/788 [==============================] - 1s 810us/step - loss: -8730958769422336.0000 - accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "788/788 [==============================] - 1s 836us/step - loss: -9277414974685184.0000 - accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "788/788 [==============================] - 1s 817us/step - loss: -9850050079358976.0000 - accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "788/788 [==============================] - 1s 795us/step - loss: -10452049875435520.0000 - accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "788/788 [==============================] - 1s 827us/step - loss: -11082315925028864.0000 - accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "788/788 [==============================] - 1s 804us/step - loss: -11742052966465536.0000 - accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "788/788 [==============================] - 1s 795us/step - loss: -12431484338044928.0000 - accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "788/788 [==============================] - 1s 810us/step - loss: -13151637610692608.0000 - accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "788/788 [==============================] - 1s 797us/step - loss: -13905822056710144.0000 - accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "788/788 [==============================] - 1s 826us/step - loss: -14692847970156544.0000 - accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "788/788 [==============================] - 1s 795us/step - loss: -15514395756986368.0000 - accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "788/788 [==============================] - 1s 819us/step - loss: -16372705242644480.0000 - accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "788/788 [==============================] - 1s 804us/step - loss: -17268393828679680.0000 - accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "788/788 [==============================] - 1s 813us/step - loss: -18201094295388160.0000 - accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "788/788 [==============================] - 1s 816us/step - loss: -19173165653557248.0000 - accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "788/788 [==============================] - 1s 793us/step - loss: -20186752165609472.0000 - accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "788/788 [==============================] - 1s 815us/step - loss: -21240762909851648.0000 - accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "788/788 [==============================] - 1s 837us/step - loss: -22341131383603200.0000 - accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "788/788 [==============================] - 1s 824us/step - loss: -23484930566651904.0000 - accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "788/788 [==============================] - 1s 932us/step - loss: -24673657255100416.0000 - accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "788/788 [==============================] - 1s 859us/step - loss: -25909130367598592.0000 - accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "788/788 [==============================] - 1s 804us/step - loss: -27192372106362880.0000 - accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "788/788 [==============================] - 1s 803us/step - loss: -28522407513817088.0000 - accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "788/788 [==============================] - 1s 797us/step - loss: -29904991846137856.0000 - accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "788/788 [==============================] - 1s 855us/step - loss: -31341819467923456.0000 - accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "788/788 [==============================] - 1s 781us/step - loss: -32831835964702720.0000 - accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "788/788 [==============================] - 1s 780us/step - loss: -34379486627627008.0000 - accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "788/788 [==============================] - 1s 779us/step - loss: -35984593215553536.0000 - accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "788/788 [==============================] - 1s 794us/step - loss: -37647900905308160.0000 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29aad66b460>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(30, kernel_initializer = 'uniform', activation = 'relu', input_dim = 47))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(15, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(20, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(10, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "classifier.fit(train_pre_X, train_pre_y, batch_size = 10, epochs = 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 621us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       ...,\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = neuralModle.predict(test_pre_X)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ac=accuracy_score(y_test, y_pred.round())\n",
    "print('accuracy of the model: ',ac)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69bf567fc4554ba01bc8aeb458947b04f6cb2a7bc167412b37d9709419993267"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
